{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Steganography using Autoencoders",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWjTIDO7skka",
        "outputId": "e8e6c7fd-829a-4b86-c043-fee53effa4c0"
      },
      "source": [
        "%tensorflow_version 1.4.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.4.1`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Cllfk-d8OQ3",
        "outputId": "951103ac-b71b-46c7-b369-512209010219"
      },
      "source": [
        "!pip install scipy==1.1.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy==1.1.0\n",
            "  Downloading scipy-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (31.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.19.5)\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lucid 0.3.10 requires umap-learn, which is not installed.\n",
            "lucid 0.3.10 requires numpy<=1.19, but you have numpy 1.19.5 which is incompatible.\n",
            "pymc3 3.11.4 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n",
            "plotnine 0.6.0 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\n",
            "jax 0.2.25 requires scipy>=1.2.1, but you have scipy 1.1.0 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed scipy-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qACfgtM87o25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8cad3aa7-af07-4453-b9f8-4a4897e3ac65"
      },
      "source": [
        "#!apt-get install unzip\n",
        "#!wget -O data.zip https://s3.eu-central-1.amazonaws.com/avg-kitti/data_road.zip\n",
        "#!unzip data.zip\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np,sys,os\n",
        "from numpy import float32\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.ndimage import imread\n",
        "from scipy.misc import imresize\n",
        "\n",
        "np.random.seed(678)\n",
        "tf.random.set_seed(678)\n",
        "\n",
        "# Activation Functions - however there was no indication in the original paper\n",
        "def tf_Relu(x): return tf.nn.relu(x)\n",
        "def d_tf_Relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
        "\n",
        "def tf_log(x): return tf.sigmoid(x)\n",
        "def d_tf_log(x): return tf_log(x) * (1.0 - tf.log(x))\n",
        "\n",
        "def tf_tanh(x): return tf.tanh(x)\n",
        "def d_tf_tanh(x): return 1.0 - tf.square(tf_tanh(x))\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    X = np.asarray(dict[b'data'].T).astype(\"uint8\")\n",
        "    Yraw = np.asarray(dict[b'labels'])\n",
        "    Y = np.zeros((10,10000))\n",
        "    for i in range(10000):\n",
        "        Y[Yraw[i],i] = 1\n",
        "    names = np.asarray(dict[b'filenames'])\n",
        "    return X,Y,names\n",
        "\n",
        "# make class \n",
        "class CNNLayer():\n",
        "    \n",
        "    def __init__(self,ker,in_c,out_c,act,d_act,):\n",
        "        \n",
        "        self.w = tf.Variable(tf.random.truncated_normal([ker,ker,in_c,out_c],stddev=0.005))\n",
        "        self.act,self.d_act = act,d_act\n",
        "        self.m,self.v = tf.Variable(tf.zeros_like(self.w)),tf.Variable(tf.zeros_like(self.w))\n",
        "\n",
        "    def feedforward(self,input,stride=1):\n",
        "        self.input  = input\n",
        "        self.layer  = tf.compat.v1.nn.conv2d(input,self.w,strides = [1,stride,stride,1],padding='SAME')\n",
        "        self.layerA = self.act(self.layer)\n",
        "        return self.layerA\n",
        "\n",
        "    def backprop(self,gradient,stride=1):\n",
        "        grad_part_1 = gradient\n",
        "        grad_part_2 = self.d_act(self.layer)\n",
        "        grad_part_3 = self.input\n",
        "\n",
        "        grad_middle = tf.multiply(grad_part_1,grad_part_2)\n",
        "        grad = tf.nn.conv2d_backprop_filter(\n",
        "            input = grad_part_3,filter_sizes = self.w.shape,\n",
        "            out_backprop = grad_middle,strides=[1,1,1,1], padding=\"SAME\"\n",
        "        )\n",
        "\n",
        "        grad_pass  = tf.nn.conv2d_backprop_input(\n",
        "            input_sizes=[batch_size] + list(self.input.shape[1:]),filter = self.w ,\n",
        "            out_backprop = grad_middle,strides=[1,1,1,1], padding=\"SAME\"\n",
        "        )\n",
        "\n",
        "        update_w = []\n",
        "\n",
        "        update_w.append(\n",
        "            tf.assign( self.m,self.m*beta_1 + (1-beta_1) * grad   )\n",
        "        )\n",
        "        update_w.append(\n",
        "            tf.assign( self.v,self.v*beta_2 + (1-beta_2) * grad ** 2   )\n",
        "        )\n",
        "\n",
        "        m_hat = self.m / (1-beta1)\n",
        "        v_hat = self.v / (1-beta2)\n",
        "        adam_middel = learning_rate/(tf.sqrt(v_hat) + adam_e)\n",
        "        update_w.append(tf.assign(self.w,tf.subtract(self.w,tf.multiply(adam_middel,m_hat))))\n",
        "\n",
        "        return grad_pass,update_w\n",
        "\n",
        "data_location = \"/content/data\"\n",
        "data_array = []  # create an empty list\n",
        "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
        "    for filename in fileList:\n",
        "        if \".jpg\" in filename.lower():  # check whether the file's DICOM\n",
        "            data_array.append(os.path.join(dirName,filename))\n",
        "\n",
        "X = np.zeros(shape=(100,80,80,3))\n",
        "\n",
        "for file_index in range(len(data_array)):\n",
        "    X[file_index,:,:]   = imresize(imread(data_array[file_index],mode='RGB'),(80,80))\n",
        "\n",
        "X[:,:,:,0] = (X[:,:,:,0]-X[:,:,:,0].min(axis=0))/(X[:,:,:,0].max(axis=0)-X[:,:,:,0].min(axis=0))\n",
        "X[:,:,:,1] = (X[:,:,:,1]-X[:,:,:,1].min(axis=0))/(X[:,:,:,1].max(axis=0)-X[:,:,:,1].min(axis=0))\n",
        "X[:,:,:,2] = (X[:,:,:,2]-X[:,:,:,2].min(axis=0))/(X[:,:,:,2].max(axis=0)-X[:,:,:,2].min(axis=0))\n",
        "\n",
        "X = shuffle(X)\n",
        "s_images = X[:50,:,:,:]\n",
        "c_images = X[50:,:,:,:]\n",
        "\n",
        "# hyper\n",
        "num_epoch = 1000\n",
        "num_epoch = 10000\n",
        "\n",
        "learing_rate = 0.0008\n",
        "batch_size = 10\n",
        "\n",
        "networ_beta = 1.0\n",
        "\n",
        "beta_1,beta_2 = 0.9,0.999\n",
        "adam_e = 1e-8\n",
        "\n",
        "# init class\n",
        "prep_net1 = CNNLayer(3,3,50,tf_Relu,d_tf_Relu)\n",
        "prep_net2 = CNNLayer(3,50,50,tf_Relu,d_tf_Relu)\n",
        "prep_net3 = CNNLayer(3,50,50,tf_Relu,d_tf_Relu)\n",
        "prep_net4 = CNNLayer(3,50,50,tf_Relu,d_tf_Relu)\n",
        "prep_net5 = CNNLayer(3,50,3,tf_Relu,d_tf_Relu)\n",
        "\n",
        "hide_net1 = CNNLayer(4,6,50,tf_Relu,d_tf_Relu)\n",
        "hide_net2 = CNNLayer(4,50,50,tf_Relu,d_tf_Relu)\n",
        "hide_net3 = CNNLayer(4,50,50,tf_Relu,d_tf_Relu)\n",
        "hide_net4 = CNNLayer(4,50,50,tf_Relu,d_tf_Relu)\n",
        "hide_net5 = CNNLayer(4,50,3,tf_Relu,d_tf_Relu)\n",
        "\n",
        "reve_net1 = CNNLayer(5,3,50,tf_Relu,d_tf_Relu)\n",
        "reve_net2 = CNNLayer(5,50,50,tf_Relu,d_tf_Relu)\n",
        "reve_net3 = CNNLayer(5,50,50,tf_Relu,d_tf_Relu)\n",
        "reve_net4 = CNNLayer(5,50,50,tf_Relu,d_tf_Relu)\n",
        "reve_net5 = CNNLayer(5,50,3,tf_Relu,d_tf_Relu)\n",
        "\n",
        "# make graph\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "Secret = tf.compat.v1.placeholder(shape=[None,80,80,3],dtype=tf.float32)\n",
        "Cover = tf.compat.v1.placeholder(shape=[None,80,80,3],dtype=tf.float32)\n",
        "\n",
        "prep_layer1 = prep_net1.feedforward(Secret)\n",
        "prep_layer2 = prep_net2.feedforward(prep_layer1)\n",
        "prep_layer3 = prep_net3.feedforward(prep_layer2)\n",
        "prep_layer4 = prep_net4.feedforward(prep_layer3)\n",
        "prep_layer5 = prep_net5.feedforward(prep_layer4)\n",
        "\n",
        "hide_Input = tf.concat([Cover,prep_layer5],axis=3)\n",
        "hide_layer1 = hide_net1.feedforward(hide_Input)\n",
        "hide_layer2 = hide_net2.feedforward(hide_layer1)\n",
        "hide_layer3 = hide_net3.feedforward(hide_layer2)\n",
        "hide_layer4 = hide_net4.feedforward(hide_layer3)\n",
        "hide_layer5 = hide_net5.feedforward(hide_layer4)\n",
        "\n",
        "reve_layer1 = reve_net1.feedforward(hide_layer5)\n",
        "reve_layer2 = reve_net2.feedforward(reve_layer1)\n",
        "reve_layer3 = reve_net3.feedforward(reve_layer2)\n",
        "reve_layer4 = reve_net4.feedforward(reve_layer3)\n",
        "reve_layer5 = reve_net5.feedforward(reve_layer4)\n",
        "\n",
        "cost_1 = tf.reduce_mean(tf.square(hide_layer5 - Cover))*0.5\n",
        "cost_2 = tf.reduce_mean(tf.square(reve_layer5 - Secret)) *0.5\n",
        "\n",
        "# --- auto train ---\n",
        "auto_train = tf.compat.v1.train.AdamOptimizer(learning_rate=learing_rate).minimize(cost_1+cost_2)\n",
        "\n",
        "\n",
        "# start the session\n",
        "with tf.compat.v1.Session() as sess : \n",
        "\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    for iter in range(num_epoch):\n",
        "        for current_batch_index in range(0,len(s_images),batch_size):\n",
        "            current_batch_s = s_images[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "            current_batch_c = c_images[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "            sess_results = sess.run([cost_1,cost_2,auto_train],feed_dict={Secret:current_batch_s,Cover:current_batch_c})\n",
        "            print(\"Iter: \",iter, ' cost 1: ',sess_results[0],' cost 2: ',sess_results[1],end='\\r')\n",
        "\n",
        "        if iter % 500 == 0 :\n",
        "            random_data_index = np.random.randint(len(s_images))\n",
        "            current_batch_s = np.expand_dims(s_images[random_data_index,:,:,:],0)\n",
        "            current_batch_c = np.expand_dims(c_images[random_data_index,:,:,:],0)\n",
        "            sess_results = sess.run([prep_layer5,hide_layer5,reve_layer5],feed_dict={Secret:current_batch_s,Cover:current_batch_c})\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(current_batch_s[0,:,:,:]))\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+' Secret')\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(current_batch_c[0,:,:,:]))\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+' cover')\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(sess_results[0][0,:,:,:]))\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+' prep image')\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(sess_results[1][0,:,:,:]))\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+\" Hidden Image \")\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure()\n",
        "            plt.axis('off')\n",
        "            plt.imshow(np.squeeze(sess_results[2][0,:,:,:]))\n",
        "            plt.title('epoch_'+str(iter)+\" Reveal  Image\")\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            plt.close('all')\n",
        "            print('\\n--------------------\\n')\n",
        "\n",
        "        if iter == num_epoch-1:\n",
        "            \n",
        "            for final in range(len(s_images)):\n",
        "                current_batch_s = np.expand_dims(s_images[final,:,:,:],0)\n",
        "                current_batch_c = np.expand_dims(c_images[final,:,:,:],0)\n",
        "                sess_results = sess.run([prep_layer5,hide_layer5,reve_layer5],feed_dict={Secret:current_batch_s,Cover:current_batch_c})\n",
        "\n",
        "                plt.figure()\n",
        "                plt.imshow(np.squeeze(current_batch_s[0,:,:,:]))\n",
        "                plt.axis('off')\n",
        "                plt.title('epoch_'+str(final)+' Secret')\n",
        "                plt.show()\n",
        "\n",
        "                plt.figure()\n",
        "                plt.imshow(np.squeeze(current_batch_c[0,:,:,:]))\n",
        "                plt.axis('off')\n",
        "                plt.title('epoch_'+str(final)+' cover')\n",
        "                plt.show()\n",
        "\n",
        "                plt.figure()\n",
        "                plt.imshow(np.squeeze(sess_results[0][0,:,:,:]))\n",
        "                plt.axis('off')\n",
        "                plt.title('epoch_'+str(final)+' prep image')\n",
        "                plt.show()\n",
        "\n",
        "                plt.figure()\n",
        "                plt.imshow(np.squeeze(sess_results[1][0,:,:,:]))\n",
        "                plt.axis('off')\n",
        "                plt.title('epoch_'+str(final)+\" Hidden Image \")\n",
        "                plt.show()\n",
        "\n",
        "                plt.figure()\n",
        "                plt.axis('off')\n",
        "                plt.imshow(np.squeeze(sess_results[2][0,:,:,:]))\n",
        "                plt.title('epoch_'+str(final)+\" Reveal  Image\")\n",
        "                plt.show()\n",
        "\n",
        "                plt.close('all')\n",
        "# -- end code --"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bc9b7588ea51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'imread' from 'scipy.ndimage' (/usr/local/lib/python3.7/dist-packages/scipy/ndimage/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}